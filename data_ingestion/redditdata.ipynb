{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import logging.handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(\"bot\")\n",
    "log.setLevel(logging.DEBUG)\n",
    "log.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "def read_and_decode(reader, chunk_size, max_window_size, previous_chunk=None, bytes_read=0):\n",
    "\tchunk = reader.read(chunk_size)\n",
    "\tbytes_read += chunk_size\n",
    "\tif previous_chunk is not None:\n",
    "\t\tchunk = previous_chunk + chunk\n",
    "\ttry:\n",
    "\t\treturn chunk.decode()\n",
    "\texcept UnicodeDecodeError:\n",
    "\t\tif bytes_read > max_window_size:\n",
    "\t\t\traise UnicodeError(f\"Unable to decode frame after reading {bytes_read:,} bytes\")\n",
    "\t\tlog.info(f\"Decoding error with {bytes_read:,} bytes, reading another chunk\")\n",
    "\t\treturn read_and_decode(reader, chunk_size, max_window_size, chunk, bytes_read)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines_zst(file_name):\n",
    "\twith open(file_name, 'rb') as file_handle:\n",
    "\t\tbuffer = ''\n",
    "\t\treader = zstandard.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n",
    "\t\twhile True:\n",
    "\t\t\tchunk = read_and_decode(reader, 2**27, (2**29) * 2)\n",
    "\n",
    "\t\t\tif not chunk:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tlines = (buffer + chunk).split(\"\\n\")\n",
    "\n",
    "\t\t\tfor line in lines[:-1]:\n",
    "\t\t\t\tyield line, file_handle.tell()\n",
    "\n",
    "\t\t\tbuffer = lines[-1]\n",
    "\n",
    "\t\treader.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import zstandard as zstd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_posts_after_date(input_file, output_file, min_date=\"2022-01-01\"):\n",
    "    output_rows = []\n",
    "    min_dt = datetime.strptime(min_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    fieldnames = [\n",
    "        \"id\", \"title\", \"selftext\", \"text\",\n",
    "        \"score\", \"upvote_ratio\", \"num_comments\", \"created_utc\"\n",
    "    ]\n",
    "\n",
    "    with open(input_file, 'rb') as fh:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        with dctx.stream_reader(fh) as reader:\n",
    "            for chunk in tqdm(iter(lambda: reader.read(2**24), b''), desc=f\"Processing {input_file}\", unit=\"chunk\"):\n",
    "                lines = chunk.decode(\"utf-8\", errors=\"ignore\").split(\"\\n\")\n",
    "                for line in lines:\n",
    "                    if not line.strip():\n",
    "                        continue\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        created_raw = obj.get(\"created_utc\", 0)\n",
    "                        try:\n",
    "                            created_dt = datetime.utcfromtimestamp(int(float(created_raw)))\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "                        # ✅ Filter here\n",
    "                        if created_dt < min_dt:\n",
    "                            continue\n",
    "\n",
    "                        output_rows.append({\n",
    "                            \"id\": obj.get(\"id\"),\n",
    "                            \"title\": obj.get(\"title\", \"\"),\n",
    "                            \"selftext\": obj.get(\"selftext\", \"\"),\n",
    "                            \"text\": f\"{obj.get('title', '')} {obj.get('selftext', '')}\",\n",
    "                            \"score\": obj.get(\"score\", 0),\n",
    "                            \"upvote_ratio\": obj.get(\"upvote_ratio\", \"\"),\n",
    "                            \"num_comments\": obj.get(\"num_comments\", 0),\n",
    "                            \"created_utc\": created_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        })\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "    # Save to CSV\n",
    "    if output_rows:\n",
    "        with open(output_file, \"w\", newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(output_rows)\n",
    "        print(f\"✅ Saved {len(output_rows)} posts to {output_file}\")\n",
    "    else:\n",
    "        print(\"⚠️ No posts after\", min_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing wallstreetbets__submissions.zst: 3chunk [00:00,  6.23chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 1245 posts to wsb_posts_2022_onward.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stocks_submissions.zst: 57chunk [00:08,  6.84chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 107129 posts to stocks_posts_2022_onward.csv\n"
     ]
    }
   ],
   "source": [
    "extract_posts_after_date(\"wallstreetbets__submissions.zst\", \"wsb_posts_2022_onward.csv\")\n",
    "extract_posts_after_date(\"stocks_submissions.zst\", \"stocks_posts_2022_onward.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing investing_submissions.zst: 55chunk [00:07,  7.23chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 103729 posts to investing_posts_2022_onward.csv\n"
     ]
    }
   ],
   "source": [
    "extract_posts_after_date(\"investing_submissions.zst\", \"investing_posts_2022_onward.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StocksAndTrading_submissions.zst: 0chunk [00:00, ?chunk/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StocksAndTrading_submissions.zst: 9chunk [00:01,  6.51chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 15525 posts to SAT_posts_2022_onward.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing StocksInFocus_submissions.zst: 20chunk [00:02,  7.89chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No posts after 2022-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stockstobuytoday_submissions.zst: 6chunk [00:00,  7.60chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 25221 posts to STB_2022_onward.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing investingforbeginners_submissions.zst: 4chunk [00:00,  6.97chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 12477 posts to IFB_2022_onward.csv\n"
     ]
    }
   ],
   "source": [
    "extract_posts_after_date(\"StocksAndTrading_submissions.zst\", \"SAT_posts_2022_onward.csv\")\n",
    "extract_posts_after_date(\"StocksInFocus_submissions.zst\", \"SIF_posts_2022_onward.csv\")\n",
    "extract_posts_after_date(\"stockstobuytoday_submissions.zst\", \"STB_2022_onward.csv\")\n",
    "extract_posts_after_date(\"investingforbeginners_submissions.zst\", \"IFB_2022_onward.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing wallstreetbets_submissions.zst: 423chunk [00:59,  7.06chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 544513 posts to WSB_2022_onward.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing wallstreetbetscontest_submissions.zst: 1chunk [00:00, 1441.34chunk/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No posts after 2022-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_posts_after_date(\"wallstreetbets_submissions.zst\", \"WSB_2022_onward.csv\")\n",
    "extract_posts_after_date(\"wallstreetbetscontest_submissions.zst\", \"WSBC_2022_onward.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
